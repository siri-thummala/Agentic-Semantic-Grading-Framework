# Intelligent Answer Evaluation System

## Problem Statement
Manual evaluation of descriptive answers is time-consuming and subjective.  
This project proposes an explainable system that evaluates student answers
using semantic similarity and concept coverage analysis.

## Scope (Version 1)
- Text-based answers only
- Semantic similarity evaluation
- Keyword / concept coverage analysis
- Not creativity-based grading

## Evaluation Pipeline
1. Input collection
2. Reference answer selection
3. Text preprocessing
4. Semantic similarity computation
5. Concept / keyword coverage analysis
6. Score calculation
7. Feedback generation

## Mark Distribution
- Semantic similarity: 4 marks
- Keyword coverage: 4 marks
- Structure & grammar: 2 marks

## ðŸ›  Technologies and Components Used
## ðŸ”¹ Backend
- FastAPI â€“ REST API framework for handling evaluation requests
- Python 3 â€“ Core backend programming language
- Pydantic â€“ Request validation and data modeling
- Uvicorn â€“ ASGI server for running the FastAPI application
- CORS Middleware â€“ Enables secure communication between frontend and backend
- Swagger UI â€“ API testing and documentation interface
## ðŸ”¹ Frontend
- React.js â€“ Frontend library for building user interfaces
- JavaScript (ES6) â€“ Client-side logic
- HTML5 & CSS â€“ Page structure and styling
- Fetch API â€“ Used to send evaluation requests to backend API
- Node.js & npm â€“ Package management and development server

## Design Philosophy
LLMs are probabilistic and non-deterministic.  
For academic evaluation, explainability and consistency are critical.
Hence, rule-based scoring and ML similarity are used, while LLMs act
only as a support layer.

## Future Enhancements
- Handwritten answer evaluation using OCR
- Multilingual support
- Instructor-defined rubrics
